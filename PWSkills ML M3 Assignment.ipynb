{
 "cells": [
  {
   "cell_type": "raw",
   "id": "eeafd4ed-1cb0-4790-bef9-50be022f62ab",
   "metadata": {},
   "source": [
    "1. What is Simple Linear Regression?\n",
    "   Simple Linear Regression is a statistical method used to model the relationship between a dependent variable and a single independent\n",
    "   variable by fitting a linear equation to observed data.\n",
    "\n",
    "2. What are the key assumptions of Simple Linear Regression?\n",
    "* Linearity\n",
    "* Independence\n",
    "* Homoscedasticity (constant variance of errors)\n",
    "* Normality of residuals\n",
    "\n",
    "3. What does the coefficient m represent in the equation Y = mX + c?\n",
    "   The coefficient 'm' represents the slope of the line, indicating the change in the dependent variable (Y) for a one-unit change in the      independent variable (X).\n",
    "\n",
    "4. What does the intercept c represent in the equation Y = mX + c\n",
    "   The intercept 'c' represents the value of Y when X = 0. It is the point where the regression line crosses the Y-axis.\n",
    "\n",
    "5. How do we calculate the slope m in Simple Linear Regression?\n",
    "   m = Σ\\[(Xi - X̄)(Yi - Ȳ)] / Σ\\[(Xi - X̄)^2]\n",
    "\n",
    "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
    "   The least squares method minimizes the sum of the squared differences between observed values and predicted values to find the best-        fitting line.\n",
    "\n",
    "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
    "   R² measures the proportion of variance in the dependent variable that is predictable from the independent variable, ranging from 0 to 1.\n",
    "\n",
    "8. What is Multiple Linear Regression?\n",
    "   Multiple Linear Regression is a statistical technique that models the relationship between a dependent variable and two or more             independent variables using a linear equation.\n",
    "\n",
    "9. What is the main difference between Simple and Multiple Linear Regression?\n",
    "   Simple Linear Regression involves one independent variable, while Multiple Linear Regression involves two or more independent variables.\n",
    "\n",
    "10. What are the key assumptions of Multiple Linear Regression?\n",
    "* Linearity\n",
    "* Independence of errors\n",
    "* Homoscedasticity\n",
    "* Normality of errors\n",
    "* No multicollinearity\n",
    "\n",
    "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
    "    Heteroscedasticity refers to non-constant variance of residuals. It violates model assumptions, leading to inefficient estimates and        biased standard errors.\n",
    "\n",
    "12. How can you improve a Multiple Linear Regression model with high multicollinearity\n",
    "* Remove highly correlated predictors\n",
    "* Use Principal Component Analysis (PCA)\n",
    "* Apply regularization techniques like Ridge or Lasso regression\n",
    "\n",
    "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
    "\n",
    "* One-Hot Encoding\n",
    "* Label Encoding\n",
    "* Binary Encoding\n",
    "* Ordinal Encoding\n",
    "\n",
    "14. What is the role of interaction terms in Multiple Linear Regression\n",
    "    Interaction terms allow modeling of the combined effect of two or more variables on the dependent variable, capturing more complex          relationships.\n",
    "\n",
    "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
    "    In Simple Linear Regression, the intercept is the expected value of Y when X is 0. In Multiple Linear Regression, it's the expected         value of Y when all X variables are 0.\n",
    "\n",
    "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
    "    The slope indicates the direction and magnitude of the relationship between a predictor and the response variable. A larger slope leads     to larger predicted changes in Y.\n",
    "\n",
    "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
    "    The intercept sets the baseline value of the dependent variable and helps interpret the impact of predictors when their values are zero.\n",
    "\n",
    "18. What are the limitations of using R² as a sole measure of model performance\\\n",
    "* It doesn't indicate causality\n",
    "* Sensitive to outliers\n",
    "* Can be artificially high in overfitted models\n",
    "* Doesn't measure model bias or variance\n",
    "\n",
    "19. How would you interpret a large standard error for a regression coefficient?\n",
    "    A large standard error suggests that the coefficient is estimated with low precision and might not be significantly different from zero.\n",
    "\n",
    "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
    "    It appears as a funnel or pattern in residual vs. fitted plots. Addressing it ensures accurate confidence intervals and hypothesis tests.\n",
    "\n",
    "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
    "    It indicates overfitting. The model includes irrelevant predictors that don't improve generalization.\n",
    "\n",
    "22. Why is it important to scale variables in Multiple Linear Regression?\n",
    "    Scaling ensures comparability of coefficients, especially when using regularization, and improves numerical stability.\n",
    "\n",
    "23. What is polynomial regression?\n",
    "    Polynomial regression is a type of regression that models the relationship between the independent variable and the dependent variable      as an nth-degree polynomial.\n",
    "\n",
    "24. How does polynomial regression differ from linear regression?\n",
    "    Polynomial regression fits a nonlinear curve to the data, whereas linear regression fits a straight line.\n",
    "\n",
    "25. When is polynomial regression used?\n",
    "    It is used when the relationship between variables is nonlinear but can be approximated using polynomial terms.\n",
    "\n",
    "26. What is the general equation for polynomial regression?\n",
    "    Y = b0 + b1X + b2X^2 + ... + bnX^n\n",
    "\n",
    "27. Can polynomial regression be applied to multiple variables?\n",
    "    Yes, by including polynomial terms of multiple independent variables and their interactions.\n",
    "\n",
    "28. What are the limitations of polynomial regression?**\n",
    "\n",
    "* Prone to overfitting\n",
    "* Extrapolation is unreliable\n",
    "* Sensitive to outliers\n",
    "* Computationally intensive for high degrees\n",
    "\n",
    "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
    "\n",
    "* Cross-validation\n",
    "* Adjusted R²\n",
    "* AIC/BIC\n",
    "* Residual analysis\n",
    "\n",
    "30. Why is visualization important in polynomial regression?\n",
    "    Visualization helps assess model fit, identify overfitting, and communicate the shape of the relationship.\n",
    "\n",
    "31. How is polynomial regression implemented in Python?\n",
    "    Using `PolynomialFeatures` from `sklearn.preprocessing` and fitting a `LinearRegression` model on the transformed features.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
